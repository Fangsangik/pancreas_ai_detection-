#!/usr/bin/env python3
"""
ì·Œì¥ CT ë°ì´í„° ì¤€ë¹„ ìŠ¤í¬ë¦½íŠ¸
==========================

NIfTI í¬ë§·ìœ¼ë¡œ ë³€í™˜ëœ CT ë°ì´í„°ë¥¼ ê²€ì¦í•˜ê³ ,
í•™ìŠµ/ê²€ì¦/í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¡œ ë¶„í• í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    python prepare_pancreas_data.py --input ./data/nifti --output ./data/processed
"""

import os
import sys
import argparse
from pathlib import Path
from typing import Dict, List, Tuple
import json
import numpy as np


def check_dependencies():
    """í•„ìš”í•œ íŒ¨í‚¤ì§€ í™•ì¸"""
    try:
        import nibabel as nib
        return True
    except ImportError:
        print("âŒ nibabel íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("   pip install nibabel")
        sys.exit(1)


def load_nifti_info(nifti_path: str) -> Dict:
    """
    NIfTI íŒŒì¼ ì •ë³´ ë¡œë“œ

    Args:
        nifti_path (str): NIfTI íŒŒì¼ ê²½ë¡œ

    Returns:
        Dict: íŒŒì¼ ì •ë³´
    """
    import nibabel as nib

    try:
        img = nib.load(nifti_path)
        data = img.get_fdata()

        return {
            "path": nifti_path,
            "shape": data.shape,
            "spacing": tuple(float(v) for v in img.header.get_zooms()),
            "dtype": str(data.dtype),
            "min_value": float(np.min(data)),
            "max_value": float(np.max(data)),
            "mean_value": float(np.mean(data)),
            "std_value": float(np.std(data)),
            "valid": True,
            "error": None
        }
    except Exception as e:
        return {
            "path": nifti_path,
            "valid": False,
            "error": str(e)
        }


def validate_ct_data(nifti_dir: str) -> Tuple[List[Dict], List[Dict]]:
    """
    CT ë°ì´í„° ê²€ì¦

    Args:
        nifti_dir (str): NIfTI íŒŒì¼ ë””ë ‰í† ë¦¬

    Returns:
        Tuple[List[Dict], List[Dict]]: (ìœ íš¨í•œ íŒŒì¼ë“¤, ì˜¤ë¥˜ íŒŒì¼ë“¤)
    """
    print("ğŸ” CT ë°ì´í„° ê²€ì¦ ì¤‘...")

    valid_files = []
    invalid_files = []

    nifti_files = list(Path(nifti_dir).glob("*.nii.gz"))

    if not nifti_files:
        print(f"âŒ NIfTI íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {nifti_dir}")
        return [], []

    print(f"   ì´ {len(nifti_files)}ê°œ íŒŒì¼ ë°œê²¬")

    for i, nifti_file in enumerate(nifti_files, 1):
        print(f"   [{i}/{len(nifti_files)}] {nifti_file.name}...", end=" ")

        info = load_nifti_info(str(nifti_file))

        if info["valid"]:
            valid_files.append(info)
            print("âœ“")
        else:
            invalid_files.append(info)
            print(f"âŒ {info['error']}")

    print(f"\nâœ“ ìœ íš¨í•œ íŒŒì¼: {len(valid_files)}")
    print(f"âŒ ì˜¤ë¥˜ íŒŒì¼: {len(invalid_files)}")

    return valid_files, invalid_files


def print_data_statistics(valid_files: List[Dict]):
    """
    ë°ì´í„° í†µê³„ ì¶œë ¥

    Args:
        valid_files (List[Dict]): ìœ íš¨í•œ íŒŒì¼ ì •ë³´ ë¦¬ìŠ¤íŠ¸
    """
    if not valid_files:
        return

    print("\nğŸ“Š ë°ì´í„° í†µê³„")
    print("=" * 70)

    # Shape í†µê³„
    shapes = [f["shape"] for f in valid_files]
    print(f"\nì˜ìƒ í¬ê¸°:")
    for shape in set(map(tuple, shapes)):
        count = shapes.count(list(shape))
        print(f"   {shape}: {count}ê°œ")

    # Spacing í†µê³„
    spacings = [f["spacing"] for f in valid_files]
    print(f"\në³µì…€ ê°„ê²© (spacing):")
    avg_spacing = np.mean(spacings, axis=0)
    print(f"   í‰ê· : ({avg_spacing[0]:.2f}, {avg_spacing[1]:.2f}, {avg_spacing[2]:.2f}) mm")

    # ê°•ë„ í†µê³„
    min_values = [f["min_value"] for f in valid_files]
    max_values = [f["max_value"] for f in valid_files]
    mean_values = [f["mean_value"] for f in valid_files]

    print(f"\nê°•ë„ ê°’ ë²”ìœ„:")
    print(f"   ìµœì†Œê°’ ë²”ìœ„: [{np.min(min_values):.1f}, {np.max(min_values):.1f}]")
    print(f"   ìµœëŒ€ê°’ ë²”ìœ„: [{np.min(max_values):.1f}, {np.max(max_values):.1f}]")
    print(f"   í‰ê· ê°’ ë²”ìœ„: [{np.min(mean_values):.1f}, {np.max(mean_values):.1f}]")

    print("=" * 70)


def create_data_split(
    valid_files: List[Dict],
    train_ratio: float = 0.7,
    val_ratio: float = 0.15,
    test_ratio: float = 0.15,
    random_seed: int = 42
) -> Dict[str, List[Dict]]:
    """
    ë°ì´í„°ë¥¼ í•™ìŠµ/ê²€ì¦/í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¡œ ë¶„í• 

    Args:
        valid_files (List[Dict]): ìœ íš¨í•œ íŒŒì¼ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        train_ratio (float): í•™ìŠµ ì„¸íŠ¸ ë¹„ìœ¨
        val_ratio (float): ê²€ì¦ ì„¸íŠ¸ ë¹„ìœ¨
        test_ratio (float): í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ë¹„ìœ¨
        random_seed (int): ëœë¤ ì‹œë“œ

    Returns:
        Dict[str, List[Dict]]: ë¶„í• ëœ ë°ì´í„°
    """
    np.random.seed(random_seed)

    # íŒŒì¼ ë¦¬ìŠ¤íŠ¸ ì…”í”Œ
    indices = np.random.permutation(len(valid_files))

    # ë¶„í•  ì¸ë±ìŠ¤ ê³„ì‚°
    n_train = int(len(valid_files) * train_ratio)
    n_val = int(len(valid_files) * val_ratio)

    train_indices = indices[:n_train]
    val_indices = indices[n_train:n_train + n_val]
    test_indices = indices[n_train + n_val:]

    return {
        "train": [valid_files[i] for i in train_indices],
        "val": [valid_files[i] for i in val_indices],
        "test": [valid_files[i] for i in test_indices]
    }


def save_data_manifest(
    data_split: Dict[str, List[Dict]],
    output_dir: str,
    dataset_name: str = "pancreas_ct"
):
    """
    ë°ì´í„° ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ì €ì¥

    Args:
        data_split (Dict): ë¶„í• ëœ ë°ì´í„°
        output_dir (str): ì¶œë ¥ ë””ë ‰í† ë¦¬
        dataset_name (str): ë°ì´í„°ì…‹ ì´ë¦„
    """
    os.makedirs(output_dir, exist_ok=True)

    # ì „ì²´ ë§¤ë‹ˆí˜ìŠ¤íŠ¸
    manifest = {
        "dataset_name": dataset_name,
        "total_samples": sum(len(v) for v in data_split.values()),
        "splits": {
            k: len(v) for k, v in data_split.items()
        },
        "data": data_split
    }

    manifest_path = os.path.join(output_dir, f"{dataset_name}_manifest.json")
    with open(manifest_path, 'w') as f:
        json.dump(manifest, f, indent=2)

    print(f"âœ“ ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ì €ì¥: {manifest_path}")

    # ê° splitë³„ íŒŒì¼ ë¦¬ìŠ¤íŠ¸ ì €ì¥ (PyTorch ë°ì´í„° ë¡œë”ìš©)
    for split_name, files in data_split.items():
        split_list = []
        for file_info in files:
            # ìƒëŒ€ ê²½ë¡œë¡œ ë³€í™˜
            rel_path = Path(file_info["path"]).name

            split_list.append({
                "image": rel_path,
                # ë‚˜ì¤‘ì— ì„¸ê·¸ë©˜í…Œì´ì…˜ ë ˆì´ë¸” ê²½ë¡œ ì¶”ê°€ ê°€ëŠ¥
                # "label": rel_path.replace("_ct.nii.gz", "_seg.nii.gz")
            })

        list_path = os.path.join(output_dir, f"{dataset_name}_{split_name}.json")
        with open(list_path, 'w') as f:
            json.dump(split_list, f, indent=2)

        print(f"âœ“ {split_name} ë¦¬ìŠ¤íŠ¸ ì €ì¥: {list_path} ({len(split_list)}ê°œ)")


def main():
    parser = argparse.ArgumentParser(
        description="ì·Œì¥ CT ë°ì´í„° ì¤€ë¹„ ë° ê²€ì¦"
    )
    parser.add_argument(
        "--input",
        type=str,
        required=True,
        help="NIfTI íŒŒì¼ ë””ë ‰í† ë¦¬"
    )
    parser.add_argument(
        "--output",
        type=str,
        required=True,
        help="ì¶œë ¥ ë””ë ‰í† ë¦¬ (ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ì €ì¥)"
    )
    parser.add_argument(
        "--train-ratio",
        type=float,
        default=0.7,
        help="í•™ìŠµ ì„¸íŠ¸ ë¹„ìœ¨ (ê¸°ë³¸ê°’: 0.7)"
    )
    parser.add_argument(
        "--val-ratio",
        type=float,
        default=0.15,
        help="ê²€ì¦ ì„¸íŠ¸ ë¹„ìœ¨ (ê¸°ë³¸ê°’: 0.15)"
    )
    parser.add_argument(
        "--test-ratio",
        type=float,
        default=0.15,
        help="í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ë¹„ìœ¨ (ê¸°ë³¸ê°’: 0.15)"
    )
    parser.add_argument(
        "--random-seed",
        type=int,
        default=42,
        help="ëœë¤ ì‹œë“œ (ê¸°ë³¸ê°’: 42)"
    )

    args = parser.parse_args()

    # ì˜ì¡´ì„± í™•ì¸
    check_dependencies()

    print("=" * 70)
    print("ì·Œì¥ CT ë°ì´í„° ì¤€ë¹„")
    print("=" * 70)
    print()

    # ì…ë ¥ ë””ë ‰í† ë¦¬ í™•ì¸
    if not os.path.exists(args.input):
        print(f"âŒ ì…ë ¥ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.input}")
        sys.exit(1)

    # ë°ì´í„° ê²€ì¦
    valid_files, invalid_files = validate_ct_data(args.input)

    if not valid_files:
        print("âŒ ìœ íš¨í•œ CT ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        sys.exit(1)

    # í†µê³„ ì¶œë ¥
    print_data_statistics(valid_files)

    # ë°ì´í„° ë¶„í• 
    print("\nğŸ“‚ ë°ì´í„° ë¶„í•  ì¤‘...")
    data_split = create_data_split(
        valid_files,
        train_ratio=args.train_ratio,
        val_ratio=args.val_ratio,
        test_ratio=args.test_ratio,
        random_seed=args.random_seed
    )

    print(f"   í•™ìŠµ: {len(data_split['train'])}ê°œ")
    print(f"   ê²€ì¦: {len(data_split['val'])}ê°œ")
    print(f"   í…ŒìŠ¤íŠ¸: {len(data_split['test'])}ê°œ")

    # ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ì €ì¥
    print("\nğŸ’¾ ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ì €ì¥ ì¤‘...")
    save_data_manifest(data_split, args.output)

    print("\n" + "=" * 70)
    print("âœ“ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ!")
    print("=" * 70)
    print("\në‹¤ìŒ ë‹¨ê³„:")
    print("1. ì„¸ê·¸ë©˜í…Œì´ì…˜ ë ˆì´ë¸” ì¤€ë¹„ (ìˆ˜ë™ ì–´ë…¸í…Œì´ì…˜ ë˜ëŠ” ê¸°ì¡´ ë ˆì´ë¸”)")
    print("2. ë°ì´í„° ë§¤ë‹ˆí˜ìŠ¤íŠ¸ì— ë ˆì´ë¸” ê²½ë¡œ ì¶”ê°€")
    print("3. í•™ìŠµ ì‹œì‘:")
    print(f"   python pancreas_cancer_diagnosis/segmentation/training/train.py \\")
    print(f"       --data-root {args.input} \\")
    print(f"       --train-list {args.output}/pancreas_ct_train.json")
    print("=" * 70)


if __name__ == "__main__":
    main()
